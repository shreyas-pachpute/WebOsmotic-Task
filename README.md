# üß† Document Intelligence RAG Chatbot System

A production-ready Retrieval-Augmented Generation (RAG) system that answers user queries grounded in the content of uploaded documents (PDF, DOCX, TXT). Built using FastAPI, LangChain, Google Gemini, ChromaDB, sentence-transformers, Tesseract OCR, and the unstructured library.

---

## üöÄ Features

- üîç **Chunking & Embedding** with `sentence-transformers/all-MiniLM-L6-v2`
- üß† **LLM Integration** using **Google Gemini (gemini-2.0-flash)**
- üìö **Vector Search** with **ChromaDB**
- üéØ **Re-ranking** using `BAAI/bge-reranker-base` for citation accuracy
- üßæ **Citation Support** for source transparency
- üìÑ **Multi-format Uploads**: PDF, DOCX, TXT (with OCR support)
---

## üîó System Architecture

User ‚Üí FastAPI ‚Üí LangChain RAG Chain
‚îú‚îÄ‚îÄ> Unstructured Parser + OCR (Tesseract)
‚îú‚îÄ‚îÄ> MiniLM Embeddings ‚Üí ChromaDB
‚îú‚îÄ‚îÄ> BGE Re-ranker (Top-K)
‚îî‚îÄ‚îÄ> Google Gemini LLM
‚Üì
Final Answer with Citations


---

## ‚öôÔ∏è Setup & Installation

### 1. Clone the repository
```bash
git clone https://github.com/your-username/document-intelligence-rag.git
cd document-intelligence-rag
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Set up .env
```bash
GEMINI_API_KEY=your_google_gemini_api_key
EMBED_MODEL=all-MiniLM-L6-v2
RE_RANKER_MODEL=BAAI/bge-reranker-base
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### 4. Start the FastAPI server
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## Technology Choice Justifications

The technology stack was selected to build a high-performance, accurate, and maintainable system, directly addressing the core requirements.

### 1. FastAPI (Backend Framework)
*   **What & Why**: A modern Python web framework.
*   **Benefit**: Chosen for its **high performance (ASGI)**, critical for responsive I/O-bound tasks (file processing, LLM calls). **Pydantic integration** ensures robust, validated API request/response cycles. **Automatic OpenAPI/ReDoc generation** directly met the deliverable for API documentation, saving significant effort.

### 2. LangChain (Orchestration)
*   **What & Why**: A framework for developing LLM-powered applications.
*   **Benefit**: **Significantly accelerated RAG pipeline development** by providing modular, pre-built components for document loading, splitting, embedding, vector store interaction, and LLM calls. Its **RAG-specific primitives** were essential for features like prompt templating and managing conversation history. We specifically leverage its up-to-date components (`langchain-core`, `langchain-huggingface`, `langchain-chroma`, etc.) for optimal functionality.

### 3. Model Stack & Its Benefits

#### a. LLM: Google Gemini (`gemini-1.5-flash-latest`)
*   **What & Why**: A powerful large language model from Google.
*   **Benefit**: Selected for its **strong reasoning, comprehension, and instruction-following capabilities**, vital for generating accurate answers based strictly on provided context. The `flash` variant offers an **optimal balance of performance and cost** for an interactive system. Its seamless integration via `langchain-google-genai` expedited development.

#### b. Embedding Model: `sentence-transformers/all-MiniLM-L6-v2`
*   **What & Why**: A highly efficient sentence embedding model.
*   **Benefit**: Chosen for its **excellent balance of retrieval performance (MTEB benchmarks) and speed/size**. This allows for fast document processing and query embedding locally, without API costs, and is crucial for effective semantic search to find relevant document chunks. Integrated via `langchain-huggingface.HuggingFaceEmbeddings`.

#### c. Re-ranker Model: `BAAI/bge-reranker-base`
*   **What & Why**: A cross-encoder model for re-ranking search results.
*   **Benefit**: **Critically improves the relevance of context fed to the LLM.** While the initial vector search finds candidate documents, the re-ranker performs a deeper semantic analysis to select the *most* relevant passages. This directly leads to **more accurate LLM answers and reduces noise**, a key project goal. Integrated via `sentence_transformers.CrossEncoder`.

### 4. Vector Database: ChromaDB
*   **What & Why**: An AI-native open-source vector database.
*   **Benefit**: Its **developer-friendliness and strong LangChain integration** (`langchain-chroma`) were key. Crucially, its **metadata filtering capabilities** are essential for retrieving chunks associated with a specific `document_id` and page numbers, enabling accurate, document-specific Q&A and citations.

### 5. Document Parsing & OCR: `unstructured` & Tesseract
*   **What & Why**: `unstructured` for parsing diverse formats; Tesseract as the OCR engine.
*   **Benefit**: Directly fulfilled the core requirements of **handling multiple document formats (PDF, DOCX, TXT)** and **performing OCR on scanned content**. `unstructured` provides a unified parsing interface and intelligently partitions documents, while Tesseract offers a robust open-source OCR solution. This combination ensures text is accurately extracted for subsequent processing.

### 6. Text Chunking Strategy: `RecursiveCharacterTextSplitter`
*   **What & Why**: A LangChain text splitter that hierarchically splits by semantic units.
*   **Benefit**: Chosen over fixed-size chunking because it **attempts to keep semantically related content (paragraphs, sentences) together.** This improves the quality of embeddings and the context provided to the LLM. The configurable `chunk_size` (1000 chars) and `overlap` (200 chars) are set to balance capturing sufficient context with embedding model limitations.


## API Documentation

The API is built using FastAPI, which automatically generates interactive documentation.

*   **Swagger UI**: Accessible at `http://localhost:8000/docs` (or `http://127.0.0.1:8000/docs`)
*   **ReDoc**: Accessible at `http://localhost:8000/redoc` (or `http://127.0.0.1:8000/redoc`)

These interfaces provide detailed information about the API endpoints, request/response schemas, and allow for direct interaction with the API for testing.

#### 1. Embed Document
*   **Endpoint**: `POST /api/embedding`
*   **Description**: Processes and embeds the provided document file.
*   **Request**: `multipart/form-data` with a `file` field containing the document (PDF, DOCX, TXT).
*   **Successful Response (200 OK)**:
    ```json
    {
      "status": "success",
      "message": "Document embedded successfully.",
      "document_id": "generated_uuid"
    }
    ```
*   **Unsuccessful Response (e.g., 400, 500)**:
    ```json
    {
      "status": "error",
      "message": "Failed to embed document.",
      "error_details": "Details of the error, e.g., 'Document content is empty.'"
    }
    ```

#### 2. Query Document
*   **Endpoint**: `POST /api/query`
*   **Description**: Allows users to query embedded documents, receive citations, and track conversation history.
*   **Request Body**:
    ```json
    {
      "query": "What is the main argument in this document?",
      "document_id": "previously_generated_uuid_from_embedding",
      "require_citations": true,
      "conversation_id": "optional_uuid_for_existing_conversation_or_null_for_new"
    }
    ```
*   **Successful Response (200 OK)** (Example for a new conversation):
    ```json
    {
      "status": "success",
      "response": {
        "answer": "The main argument is about the impact of AI on society.",
        "citations": [
          {
            "page": 12, 
            "document_name": "sample_document.pdf"
          }
        ]
      },
      "conversation_id": "newly_generated_conversation_uuid"
    }
    ```
*   **Unsuccessful Response (e.g., 404 for invalid ID, 500 for processing error)**:
    ```json
    {
      "status": "error",
      "message": "Error message, e.g., 'Invalid conversation ID. Please start a new session.'"
    }
    ```

## üìà Performance Metrics

### üóÉÔ∏è Document Ingestion & Embedding Time

- **Small TXT file** (e.g., 10KB, ~5 pages text):
  - *Processing & Chunking Time:* `~0.3 seconds`
  - *Embedding & Storage Time:* `~0.6 seconds`
  - *Total Time:* `~0.9 seconds`

- **Medium Text-based PDF** (e.g., 20 pages, ~500KB):
  - *Processing & Chunking Time:* `~1.2 seconds`
  - *Embedding & Storage Time:* `~2.1 seconds`
  - *Total Time:* `~3.3 seconds`

- **Medium Scanned PDF** (requiring OCR, e.g., 5 pages, ~2MB):
  - *Processing (including OCR) & Chunking Time:* `~6.0 seconds`
  - *Embedding & Storage Time:* `~2.5 seconds`
  - *Total Time:* `~8.5 seconds`

- **DOCX file** (e.g., 15 pages, text and a few small images):
  - *Processing (OCR for images if any) & Chunking Time:* `~2.0 seconds`
  - *Embedding & Storage Time:* `~1.9 seconds`
  - *Total Time:* `~3.9 seconds`

---

### üí¨ Query Response Time (After Documents Are Embedded)

- *Initial Retrieval (Vector Search):* `~0.05 - 0.1 seconds`
- *Re-ranking Time (Top-10 candidates):* `~0.3 - 0.6 seconds` (CPU)
- *LLM (Gemini) Generation Time:* `~1.2 - 2.0 seconds` (depending on prompt length)
- *Total Average Query Response Time:* `~1.7 - 2.7 seconds`

---

### üìä Resource Usage (Observed During Typical Load)

- *CPU Usage during Embedding (with OCR):* `60% - 95%` on **8 cores**
- *CPU Usage during Query (Re-ranking & LLM):* `40% - 75%`
- *Memory Usage (RAM):* `~600 MB - 2.5 GB`
- *Disk Space for ChromaDB:*
  - Small (<5 docs): ~20 MB
  - Medium (50+ docs): ~200+ MB
  - Scales with chunk count and embedding size

> üß† Notes:
> - OCR significantly increases latency for scanned PDFs
> - Gemini API latency varies by load and request complexity
> - CPU-only setup is sufficient for low-concurrency environments

## Testing Strategy & Document Categories

The system was tested with documents from each category specified in the problem statement to ensure robust handling and accurate information retrieval.

## üß™ Example Tested Documents

| Type        | Document Example                                                                 |
|-------------|----------------------------------------------------------------------------------|
| TXT         | AI Blog Text (Internal)                                                          |
| DOCX        | Sample Project Report (Internal)                                                 |
| PDF         | [AI Research Paper (PDF)](https://arxiv.org/abs/2305.10085)                      |

---

**General Testing Observations:**
*   Citation accuracy for page numbers was generally good when page information was available in the source document and extracted reliably by `unstructured`.
*   Conversation history effectively maintained context for follow-up questions related to the same document.
*   The re-ranking step demonstrably improved the relevance of the top contexts provided to the LLM, leading to more focused answers, especially for ambiguous queries.
